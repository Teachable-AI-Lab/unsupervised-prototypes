{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "import random\n",
    "import time\n",
    "from torchvision import transforms, datasets, models\n",
    "from torch.utils.data import DataLoader\n",
    "import PIL \n",
    "from tqdm import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "# tsne and pca\n",
    "from sklearn.manifold import TSNE\n",
    "from sklearn.decomposition import PCA\n",
    "import untils\n",
    "from MiniDeepTaxonNet import CobwebNN, CobwebNNTreeLayer, TestModel\n",
    "import argparse\n",
    "import os\n",
    "import sys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/nethome/zwang910/miniconda3/envs/dl/lib/python3.13/site-packages/torch/utils/data/dataloader.py:624: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# load mnist\n",
    "download = True\n",
    "dataset_class = datasets.MNIST\n",
    "mnist_transform = [transforms.ToTensor()]\n",
    "# mnist_transform.append(transforms.Normalize((0.1307,), (0.3081,)))\n",
    "\n",
    "dataset_transform = transforms.Compose(mnist_transform)\n",
    "mnist_train = dataset_class('data/MNIST', train=True, download=download, transform=dataset_transform)\n",
    "mnist_test = dataset_class('data/MNIST', train=False, download=download, transform=dataset_transform)\n",
    "\n",
    "mnist_train_loader = DataLoader(mnist_train, batch_size=128, shuffle=True, num_workers=4, pin_memory=True)\n",
    "mnist_test_loader = DataLoader(mnist_test, batch_size=128, shuffle=True, num_workers=4, pin_memory=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "strong_transform = transforms.Compose([\n",
    "    # Random rotation between -30 and 30 degrees.\n",
    "    transforms.RandomRotation(degrees=30),\n",
    "    # Random affine: random translation, scaling, and shear.\n",
    "    transforms.RandomAffine(\n",
    "        degrees=90,                # no additional rotation here\n",
    "        translate=(0.4, 0.4),     # translate up to 20% of image size\n",
    "        scale=(0.4, 1.7),         # scale between 80% and 120%\n",
    "        shear=50                 # shear by up to 20 degrees\n",
    "    ),\n",
    "    # radnom crop\n",
    "    transforms.RandomCrop(28, padding=4),\n",
    "    # Random horizontal flip.\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    # Randomly adjust brightness and contrast.\n",
    "    # transforms.ColorJitter(brightness=0.5, contrast=0.5),\n",
    "    transforms.ToTensor(),\n",
    "])\n",
    "no_transform = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "])\n",
    "\n",
    "random_noise = transforms.Compose([\n",
    "    transforms.Lambda(lambda x: x + torch.randn_like(x) * 0.2),\n",
    "])\n",
    "def simclr_transform(x):\n",
    "    # We apply 'train_transform' twice to get two distinct views\n",
    "    # from the same original image.\n",
    "    return no_transform(x), strong_transform(x)\n",
    "\n",
    "train_dataset = datasets.MNIST(\n",
    "    root='./data',\n",
    "    train=True,\n",
    "    download=True,\n",
    "    # We'll override how __getitem__ works to produce two augmented views\n",
    "    transform=None\n",
    ")\n",
    "# Because we want each sample to return a pair (x1, x2), we define a custom Dataset wrapper:\n",
    "class SimCLRDatasetWrapper(torch.utils.data.Dataset):\n",
    "    def __init__(self, dataset, transform_fn):\n",
    "        self.dataset = dataset\n",
    "        self.transform_fn = transform_fn\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        img, label = self.dataset[idx]\n",
    "        # Apply our simclr_transform to get x1, x2\n",
    "        x1, x2 = self.transform_fn(img)\n",
    "        # add random noise to x2\n",
    "        # x2 = random_noise(x2)\n",
    "        return x1, x2, label  # we can return the original label if needed (not used in unsupervised training)\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.dataset)\n",
    "\n",
    "train_dataset_simclr = SimCLRDatasetWrapper(train_dataset, simclr_transform)\n",
    "mnist_simclr_train_loader = DataLoader(train_dataset_simclr, batch_size=128, shuffle=True, num_workers=4, pin_memory=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "class VGGLikeMNIST(nn.Module):\n",
    "    def __init__(self, num_classes=10):\n",
    "        super(VGGLikeMNIST, self).__init__()\n",
    "        self.features = nn.Sequential(\n",
    "            # Block 1: 28x28 -> 14x14\n",
    "            nn.Conv2d(1, 64, kernel_size=3, padding=1),  # Input: 1 channel, Output: 64\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(64, 64, kernel_size=3, padding=1),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2),\n",
    "            \n",
    "            # Block 2: 14x14 -> 7x7\n",
    "            nn.Conv2d(64, 128, kernel_size=3, padding=1),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(128, 128, kernel_size=3, padding=1),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2),\n",
    "            \n",
    "            # Block 3: 7x7 -> 3x3 (7//2 = 3 using floor division)\n",
    "            nn.Conv2d(128, 256, kernel_size=3, padding=1),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(256, 256, kernel_size=3, padding=1),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "        )\n",
    "        \n",
    "        # After three pooling layers, the spatial size becomes roughly 3x3 (if using floor division).\n",
    "        # Flatten the feature map and use a simple classifier.\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Linear(256 * 3 * 3, 1024),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Dropout(0.5),\n",
    "            nn.Linear(1024, num_classes)\n",
    "        )\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.features(x)\n",
    "        # Flatten the feature map into a vector for the classifier\n",
    "        x = x.view(x.size(0), -1)\n",
    "        # x = self.classifier(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "class IIC(nn.Module):\n",
    "    def __init__(self, n_classes):\n",
    "        super(IIC, self).__init__()\n",
    "        self.n_classes = n_classes\n",
    "        # CNN encoder for MNIST\n",
    "        # self.encoder = nn.Sequential(\n",
    "        #     nn.Conv2d(1, 32, 3, 1),\n",
    "        #     nn.ReLU(),\n",
    "        #     nn.Conv2d(32, 64, 3, 1),\n",
    "        #     nn.ReLU(),\n",
    "        #     nn.MaxPool2d(2),\n",
    "        #     nn.Conv2d(64, 128, 3, 1),\n",
    "        #     nn.ReLU(),\n",
    "        #     nn.MaxPool2d(2),\n",
    "        #     nn.Conv2d(128, 256, 3, 1),\n",
    "        #     nn.ReLU()\n",
    "        # ) # output shape: 256 x 3 x 3\n",
    "        self.encoder = VGGLikeMNIST()\n",
    "        # drop the projection head\n",
    "        # projection head\n",
    "        self.projection_head = nn.Sequential(\n",
    "            nn.Linear(256 * 3 * 3, 1024),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Dropout(0.5),\n",
    "            nn.Linear(1024, n_classes)\n",
    "        )\n",
    "        # # clustering head\n",
    "        # self.cluster_head = nn.Sequential(\n",
    "        #     nn.Linear(64, n_classes)\n",
    "        # )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.encoder(x)\n",
    "        # print(x.shape)\n",
    "        # x = x.view(x.size(0), -1)\n",
    "        x = self.projection_head(x)\n",
    "        # x = F.softmax(self.cluster_head(x), dim=1) # batch x n_classes\n",
    "        x = F.softmax(x, dim=1)\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/469 [00:00<?, ?it/s]/nethome/zwang910/miniconda3/envs/dl/lib/python3.13/site-packages/torch/utils/data/dataloader.py:624: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
      "  warnings.warn(\n",
      "100%|██████████| 469/469 [00:17<00:00, 26.27it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10, Loss: 5.437214056769335e-08\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 17%|█▋        | 81/469 [00:03<00:15, 24.97it/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[28]\u001b[39m\u001b[32m, line 32\u001b[39m\n\u001b[32m     29\u001b[39m     plt.savefig(\u001b[33mf\u001b[39m\u001b[33m'\u001b[39m\u001b[33mIIC.png\u001b[39m\u001b[33m'\u001b[39m)\n\u001b[32m     30\u001b[39m     plt.close()\n\u001b[32m---> \u001b[39m\u001b[32m32\u001b[39m \u001b[43m\u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m_\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mtqdm\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmnist_simclr_train_loader\u001b[49m\u001b[43m)\u001b[49m\u001b[43m:\u001b[49m\n\u001b[32m     33\u001b[39m \u001b[43m    \u001b[49m\u001b[43mx\u001b[49m\u001b[43m \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[43mx\u001b[49m\u001b[43m.\u001b[49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     34\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;66;43;03m# display x\u001b[39;49;00m\n\u001b[32m     35\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;66;43;03m# plt.imshow(x[0].squeeze().cpu().numpy())\u001b[39;49;00m\n\u001b[32m     36\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;66;43;03m# random augmentation\u001b[39;49;00m\n\u001b[32m     37\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;66;43;03m#### Your code here ####\u001b[39;49;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/dl/lib/python3.13/site-packages/tqdm/std.py:1181\u001b[39m, in \u001b[36mtqdm.__iter__\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m   1178\u001b[39m time = \u001b[38;5;28mself\u001b[39m._time\n\u001b[32m   1180\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1181\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mobj\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43miterable\u001b[49m\u001b[43m:\u001b[49m\n\u001b[32m   1182\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43;01myield\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mobj\u001b[49m\n\u001b[32m   1183\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;66;43;03m# Update and possibly print the progressbar.\u001b[39;49;00m\n\u001b[32m   1184\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;66;43;03m# Note: does not call self.update(1) for speed optimisation.\u001b[39;49;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/dl/lib/python3.13/site-packages/torch/utils/data/dataloader.py:708\u001b[39m, in \u001b[36m_BaseDataLoaderIter.__next__\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    705\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._sampler_iter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    706\u001b[39m     \u001b[38;5;66;03m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[32m    707\u001b[39m     \u001b[38;5;28mself\u001b[39m._reset()  \u001b[38;5;66;03m# type: ignore[call-arg]\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m708\u001b[39m data = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_next_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    709\u001b[39m \u001b[38;5;28mself\u001b[39m._num_yielded += \u001b[32m1\u001b[39m\n\u001b[32m    710\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[32m    711\u001b[39m     \u001b[38;5;28mself\u001b[39m._dataset_kind == _DatasetKind.Iterable\n\u001b[32m    712\u001b[39m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m._IterableDataset_len_called \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m    713\u001b[39m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m._num_yielded > \u001b[38;5;28mself\u001b[39m._IterableDataset_len_called\n\u001b[32m    714\u001b[39m ):\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/dl/lib/python3.13/site-packages/torch/utils/data/dataloader.py:1458\u001b[39m, in \u001b[36m_MultiProcessingDataLoaderIter._next_data\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m   1455\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._process_data(data)\n\u001b[32m   1457\u001b[39m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m._shutdown \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m._tasks_outstanding > \u001b[32m0\u001b[39m\n\u001b[32m-> \u001b[39m\u001b[32m1458\u001b[39m idx, data = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_get_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1459\u001b[39m \u001b[38;5;28mself\u001b[39m._tasks_outstanding -= \u001b[32m1\u001b[39m\n\u001b[32m   1460\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._dataset_kind == _DatasetKind.Iterable:\n\u001b[32m   1461\u001b[39m     \u001b[38;5;66;03m# Check for _IterableDatasetStopIteration\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/dl/lib/python3.13/site-packages/torch/utils/data/dataloader.py:1410\u001b[39m, in \u001b[36m_MultiProcessingDataLoaderIter._get_data\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m   1408\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._pin_memory:\n\u001b[32m   1409\u001b[39m     \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28mself\u001b[39m._pin_memory_thread.is_alive():\n\u001b[32m-> \u001b[39m\u001b[32m1410\u001b[39m         success, data = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_try_get_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1411\u001b[39m         \u001b[38;5;28;01mif\u001b[39;00m success:\n\u001b[32m   1412\u001b[39m             \u001b[38;5;28;01mreturn\u001b[39;00m data\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/dl/lib/python3.13/site-packages/torch/utils/data/dataloader.py:1251\u001b[39m, in \u001b[36m_MultiProcessingDataLoaderIter._try_get_data\u001b[39m\u001b[34m(self, timeout)\u001b[39m\n\u001b[32m   1238\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m_try_get_data\u001b[39m(\u001b[38;5;28mself\u001b[39m, timeout=_utils.MP_STATUS_CHECK_INTERVAL):\n\u001b[32m   1239\u001b[39m     \u001b[38;5;66;03m# Tries to fetch data from `self._data_queue` once for a given timeout.\u001b[39;00m\n\u001b[32m   1240\u001b[39m     \u001b[38;5;66;03m# This can also be used as inner loop of fetching without timeout, with\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m   1248\u001b[39m     \u001b[38;5;66;03m# Returns a 2-tuple:\u001b[39;00m\n\u001b[32m   1249\u001b[39m     \u001b[38;5;66;03m#   (bool: whether successfully get data, any: data if successful else None)\u001b[39;00m\n\u001b[32m   1250\u001b[39m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1251\u001b[39m         data = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_data_queue\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1252\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m (\u001b[38;5;28;01mTrue\u001b[39;00m, data)\n\u001b[32m   1253\u001b[39m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m   1254\u001b[39m         \u001b[38;5;66;03m# At timeout and error, we manually check whether any worker has\u001b[39;00m\n\u001b[32m   1255\u001b[39m         \u001b[38;5;66;03m# failed. Note that this is the only mechanism for Windows to detect\u001b[39;00m\n\u001b[32m   1256\u001b[39m         \u001b[38;5;66;03m# worker failures.\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/dl/lib/python3.13/queue.py:213\u001b[39m, in \u001b[36mQueue.get\u001b[39m\u001b[34m(self, block, timeout)\u001b[39m\n\u001b[32m    211\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m remaining <= \u001b[32m0.0\u001b[39m:\n\u001b[32m    212\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m Empty\n\u001b[32m--> \u001b[39m\u001b[32m213\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mnot_empty\u001b[49m\u001b[43m.\u001b[49m\u001b[43mwait\u001b[49m\u001b[43m(\u001b[49m\u001b[43mremaining\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    214\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.is_shutdown \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m._qsize():\n\u001b[32m    215\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m ShutDown\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/dl/lib/python3.13/threading.py:363\u001b[39m, in \u001b[36mCondition.wait\u001b[39m\u001b[34m(self, timeout)\u001b[39m\n\u001b[32m    361\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    362\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m timeout > \u001b[32m0\u001b[39m:\n\u001b[32m--> \u001b[39m\u001b[32m363\u001b[39m         gotit = \u001b[43mwaiter\u001b[49m\u001b[43m.\u001b[49m\u001b[43macquire\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    364\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    365\u001b[39m         gotit = waiter.acquire(\u001b[38;5;28;01mFalse\u001b[39;00m)\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "# train on MNIST\n",
    "\n",
    "epochs = 10\n",
    "n_classes = 10\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model = IIC(n_classes).to(device)\n",
    "\n",
    "optimizer = optim.Adam(model.parameters(), lr=1e-3)\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    model.train()\n",
    "    if epoch % 2 == 0:\n",
    "        # tsne on test set\n",
    "        test_features = []\n",
    "        test_labels = []\n",
    "        for x, y in mnist_test_loader:\n",
    "            x = x.to(device)\n",
    "            z = model.encoder(x)\n",
    "            z = z.view(z.size(0), -1)\n",
    "            test_features.append(z.detach().cpu().numpy())\n",
    "            test_labels.append(y.detach().cpu().numpy())\n",
    "            break\n",
    "        test_features = np.concatenate(test_features, axis=0)\n",
    "        test_labels = np.concatenate(test_labels, axis=0)\n",
    "        tsne = TSNE(n_components=2)\n",
    "        tsne_features = tsne.fit_transform(test_features)\n",
    "        plt.figure()\n",
    "        plt.scatter(tsne_features[:, 0], tsne_features[:, 1], c=test_labels, cmap='tab10')\n",
    "        plt.savefig(f'IIC.png')\n",
    "        plt.close()\n",
    "\n",
    "    for x, gx, _ in tqdm(mnist_simclr_train_loader):\n",
    "        x = x.to(device)\n",
    "        # display x\n",
    "        # plt.imshow(x[0].squeeze().cpu().numpy())\n",
    "        # random augmentation\n",
    "        #### Your code here ####\n",
    "        gx = gx.to(device)\n",
    "\n",
    "        # display gx\n",
    "        # plt.imshow(gx[0].squeeze().cpu().numpy())\n",
    "        # break\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # forward pass\n",
    "        z = model(x)\n",
    "        zt = model(gx)\n",
    "        # print(z, zt)\n",
    "        P = (z.unsqueeze(2) * zt.unsqueeze(1)).sum(dim=0) # n_classes x n_classes\n",
    "        # print(P)\n",
    "        P = ((P + P.t()) / 2) / P.sum()\n",
    "        # print(P)\n",
    "        # break\n",
    "        eps = 1e-5\n",
    "        P[(P < eps)] = eps\n",
    "        Pi = P.sum(dim=1).view(-1, 1).expand(n_classes, n_classes)\n",
    "        # clamp Pi\n",
    "        Pi = torch.clamp(Pi, min=eps)\n",
    "        # print(Pi)\n",
    "        Pj = P.sum(dim=0).view(1, -1).expand(n_classes, n_classes)\n",
    "        Pj = torch.clamp(Pj, min=eps)\n",
    "        # Pj[(Pj < eps)] = eps\n",
    "        # print(Pj)\n",
    "        # break\n",
    "        # loss = -(P * (torch.log(Pi) + torch.log(Pj) - torch.log(P))).sum()\n",
    "        loss = -(P * (torch.log(P) - torch.log(Pi) - torch.log(Pj))).sum()\n",
    "        # break\n",
    "        # print(loss.item())\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "    print(f'Epoch {epoch+1}/{epochs}, Loss: {loss.item()}')\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.09375\n"
     ]
    }
   ],
   "source": [
    "# test the assignment probability\n",
    "model.eval()\n",
    "test_features = []\n",
    "test_labels = []\n",
    "for x, y in mnist_test_loader:\n",
    "    x = x.to(device)\n",
    "    z = model(x)\n",
    "    y_pred = z.argmax(dim=1).detach().cpu().numpy()\n",
    "    acc = (y_pred == y).float().mean()\n",
    "    print(f'Accuracy: {acc.item()}')\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dl",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
