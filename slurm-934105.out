no change     /nethome/zwang910/miniconda3/condabin/conda
no change     /nethome/zwang910/miniconda3/bin/conda
no change     /nethome/zwang910/miniconda3/bin/conda-env
no change     /nethome/zwang910/miniconda3/bin/activate
no change     /nethome/zwang910/miniconda3/bin/deactivate
no change     /nethome/zwang910/miniconda3/etc/profile.d/conda.sh
no change     /nethome/zwang910/miniconda3/etc/fish/conf.d/conda.fish
no change     /nethome/zwang910/miniconda3/shell/condabin/Conda.psm1
no change     /nethome/zwang910/miniconda3/shell/condabin/conda-hook.ps1
no change     /nethome/zwang910/miniconda3/lib/python3.12/site-packages/xontrib/conda.xsh
no change     /nethome/zwang910/miniconda3/etc/profile.d/conda.csh
no change     /nethome/zwang910/.bashrc
No action taken.
wandb: Using wandb-core as the SDK backend.  Please refer to https://wandb.me/wandb-core for more information.
wandb: Currently logged in as: ndrsn0208 (ndrsn0208-georgia-institute-of-technology) to https://api.wandb.ai. Use `wandb login --relogin` to force relogin
wandb: Tracking run with wandb version 0.19.8
wandb: Run data is saved locally in /nethome/zwang910/research/unsupervised-prototypes/wandb/run-20250430_211600-mk7v6ouz
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run dtn-10-t-0.5-lrscheduler-cifar100
wandb: ‚≠êÔ∏è View project at https://wandb.ai/ndrsn0208-georgia-institute-of-technology/deep-taxon
wandb: üöÄ View run at https://wandb.ai/ndrsn0208-georgia-institute-of-technology/deep-taxon/runs/mk7v6ouz
Configuration loaded successfully:
------------------------------
  batch_size: 128 (Type: int)
  contrastive_loss_weight: 100.0 (Type: float)
  convex_weight_lambda: 0.02 (Type: float)
  dataset: cifar-100 (Type: str)
  dec_hidden_dim: (512, 1, 1) (Type: tuple)
  decoder_name: resnet18 (Type: str)
  device_id: 0 (Type: int)
  dkl_margin: 1.2 (Type: float)
  dkl_weight_lambda: 0.02 (Type: float)
  embed_temp: 0.5 (Type: float)
  enc_hidden_dim: 512 (Type: int)
  encoder_name: resnet18 (Type: str)
  epochs: 501 (Type: int)
  input_dim: 3072 (Type: int)
  kl1_weight: 1.0 (Type: float)
  latent_dim: 64 (Type: int)
  linear_probing_epochs: 50 (Type: int)
  linear_probing_lr: 0.01 (Type: float)
  logvar_init_range: -1 (Type: int)
  lr: 0.001 (Type: float)
  lr_scheduler: step (Type: str)
  model_save_interval: 20 (Type: int)
  model_save_path: dtn-10-t-0.5-lrscheduler-cifar100 (Type: str)
  n_classes: 100 (Type: int)
  n_layers: 10 (Type: int)
  normalize: False (Type: bool)
  pcx_temp: 0.5 (Type: float)
  pretrained_encoder: False (Type: bool)
  pretraining_epochs: 0 (Type: int)
  pretraining_lr: 0.001 (Type: float)
  recon_weight: 1.0 (Type: float)
  seed: 0 (Type: int)
  use_contrastive_loss: True (Type: bool)
  vade_baseline: False (Type: bool)
  wandb: True (Type: bool)
  wandb_project: deep-taxon (Type: str)
  wandb_run_name: dtn-10-t-0.5-lrscheduler-cifar100 (Type: str)
------------------------------
Loading data...
Start training...
Epoch 0
Model saved at /nethome/zwang910/file_storage/nips-2025/deep-taxon/project-checkin/dtn-10-t-0.5-lrscheduler-cifar100//deep_taxon_0.pt
Traceback (most recent call last):
  File "/nethome/zwang910/research/unsupervised-prototypes/train-deep-taxonnet.py", line 351, in <module>
    annotation = utils.label_annotation(model, train_loader, args.n_classes, device)
  File "/nethome/zwang910/research/unsupervised-prototypes/utils.py", line 259, in label_annotation
    pcx_c = pcx[class_indices] # shape: (N_c, n_nodes)
            ~~~^^^^^^^^^^^^^^^
IndexError: too many indices for tensor of dimension 2
[1;34mwandb[0m: 
[1;34mwandb[0m: üöÄ View run [33mdtn-10-t-0.5-lrscheduler-cifar100[0m at: [34mhttps://wandb.ai/ndrsn0208-georgia-institute-of-technology/deep-taxon/runs/mk7v6ouz[0m
[1;34mwandb[0m: Find logs at: [1;35mwandb/run-20250430_211600-mk7v6ouz/logs[0m
