no change     /nethome/zwang910/miniconda3/condabin/conda
no change     /nethome/zwang910/miniconda3/bin/conda
no change     /nethome/zwang910/miniconda3/bin/conda-env
no change     /nethome/zwang910/miniconda3/bin/activate
no change     /nethome/zwang910/miniconda3/bin/deactivate
no change     /nethome/zwang910/miniconda3/etc/profile.d/conda.sh
no change     /nethome/zwang910/miniconda3/etc/fish/conf.d/conda.fish
no change     /nethome/zwang910/miniconda3/shell/condabin/Conda.psm1
no change     /nethome/zwang910/miniconda3/shell/condabin/conda-hook.ps1
no change     /nethome/zwang910/miniconda3/lib/python3.12/site-packages/xontrib/conda.xsh
no change     /nethome/zwang910/miniconda3/etc/profile.d/conda.csh
no change     /nethome/zwang910/.bashrc
No action taken.
wandb: Using wandb-core as the SDK backend.  Please refer to https://wandb.me/wandb-core for more information.
wandb: Currently logged in as: ndrsn0208 (ndrsn0208-georgia-institute-of-technology) to https://api.wandb.ai. Use `wandb login --relogin` to force relogin
wandb: Tracking run with wandb version 0.19.8
wandb: Run data is saved locally in /nethome/zwang910/research/unsupervised-prototypes/wandb/run-20250430_205805-8ixzwnoz
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run dtn-10-t0.5-stepLR-stl10-baseline
wandb: ‚≠êÔ∏è View project at https://wandb.ai/ndrsn0208-georgia-institute-of-technology/deep-taxon
wandb: üöÄ View run at https://wandb.ai/ndrsn0208-georgia-institute-of-technology/deep-taxon/runs/8ixzwnoz
Configuration loaded successfully:
------------------------------
  batch_size: 128 (Type: int)
  contrastive_loss_weight: 100.0 (Type: float)
  convex_weight_lambda: 0.02 (Type: float)
  dataset: stl-10 (Type: str)
  dec_hidden_dim: (512, 3, 3) (Type: tuple)
  decoder_name: resnet18 (Type: str)
  device_id: 0 (Type: int)
  dkl_margin: 1.2 (Type: float)
  dkl_weight_lambda: 0.02 (Type: float)
  embed_temp: 0.5 (Type: float)
  enc_hidden_dim: 4608 (Type: int)
  encoder_name: resnet18 (Type: str)
  epochs: 501 (Type: int)
  input_dim: 3072 (Type: int)
  kl1_weight: 1.0 (Type: float)
  latent_dim: 64 (Type: int)
  linear_probing_epochs: 50 (Type: int)
  linear_probing_lr: 0.01 (Type: float)
  logvar_init_range: -1 (Type: int)
  lr: 0.001 (Type: float)
  lr_scheduler: step (Type: str)
  model_save_interval: 20 (Type: int)
  model_save_path: dtn-10-t0.5-stepLR-stl10-baseline (Type: str)
  n_classes: 10 (Type: int)
  n_layers: 11 (Type: int)
  normalize: False (Type: bool)
  pcx_temp: 0.5 (Type: float)
  pretrained_encoder: False (Type: bool)
  pretraining_epochs: 0 (Type: int)
  pretraining_lr: 0.001 (Type: float)
  recon_weight: 1.0 (Type: float)
  seed: 0 (Type: int)
  use_contrastive_loss: True (Type: bool)
  vade_baseline: True (Type: bool)
  wandb: True (Type: bool)
  wandb_project: deep-taxon (Type: str)
  wandb_run_name: dtn-10-t0.5-stepLR-stl10-baseline (Type: str)
------------------------------
Loading data...
Start training...
Epoch 0
Epoch 1
Epoch 2
Epoch 3
Epoch 4
Epoch 5
Epoch 6
Epoch 7
Epoch 8
Epoch 9
Epoch 10
Epoch 11
Epoch 12
Epoch 13
Epoch 14
Epoch 15
Epoch 16
Epoch 17
Epoch 18
Epoch 19
Epoch 20
Epoch 21
Epoch 22
Epoch 23
Epoch 24
Epoch 25
Epoch 26
Epoch 27
Epoch 28
Epoch 29
Epoch 30
Epoch 31
Epoch 32
Epoch 33
Epoch 34
Epoch 35
Epoch 36
Epoch 37
Epoch 38
Epoch 39
Epoch 40
Epoch 41
Epoch 42
Epoch 43
Epoch 44
Epoch 45
Epoch 46
Epoch 47
Epoch 48
Epoch 49
Epoch 50
Epoch 51
Epoch 52
Epoch 53
Epoch 54
Epoch 55
Epoch 56
Epoch 57
Epoch 58
Epoch 59
Epoch 60
Epoch 61
Epoch 62
Epoch 63
Epoch 64
Epoch 65
Epoch 66
Epoch 67
Epoch 68
Epoch 69
Epoch 70
Epoch 71
Epoch 72
Epoch 73
Epoch 74
Epoch 75
Epoch 76
Epoch 77
Epoch 78
Epoch 79
Epoch 80
Epoch 81
Epoch 82
Epoch 83
Epoch 84
Epoch 85
